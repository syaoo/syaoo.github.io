{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/03/30/hello-world/"},{"title":"机器学习-线性回归与逻辑回归","text":"机器学习-线性回归与逻辑回归 符号约定： \\(m\\) - 训练集中样本数量 \\(n\\) - 样本的特征数量 \\(x\\) - 输入变量 \\(\\theta\\) - 预测参数 \\(y\\) - 目标变量 \\(x_i^j\\) - 第\\(j\\)个样本中的第\\(i\\)个变量 线性回归 线性回归可发现输入变量与目标变量之间的相关关系，从而根据已知的条件来预测结果。 假设函数(hypothesis Fun) \\[ h_{\\theta}(x)=\\theta_0+\\theta_1x_1+...+\\theta_nx_n = \\sum_{i=0}^n\\theta_ix_i=\\theta^Tx \\] 式中，\\(x_0\\)=1。 代价函数(Cost Fun) \\[ J(\\theta) = \\frac{1}{2m}\\sum_{j=1}^n(h_{\\theta}(x^i)-y^j)^2 \\] 代价函数反映了在给定预测参数\\(\\theta\\)下，假设函数与实际结果的差距，为了能够得到最优的拟合结果，我们需要找到最佳拟合参数\\(\\theta\\)使得代价函数最小。 求解最优参数的方法：梯度下降法，正规方程 梯度下降法 开始时我们随机选择一个参数的组合\\((\\theta_0,\\theta_1,...,\\theta_n)\\)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（local minimum）。 批量梯度下降 (BGD, batch gradient descent) 的算法: \\[ \\begin{align} repeat :&amp;\\\\ &amp;\\theta_j := \\theta_j-\\alpha\\frac{\\partial}{\\partial{\\theta_j}}{J(\\theta)},\\ (j=0,1,...,m)\\\\ \\end{align} \\] 其中，\\(\\frac{\\partial}{\\partial{\\theta_j}}{J(\\theta)} = \\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^i)-y^i)x_j^i\\)，我们开始随机选择一组的参数，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛，方可得到最优的参数\\(\\theta\\)。 正规方程（ Normal Equation） 假设函数：\\(h_{\\theta}(X)=X\\theta\\)，代价函数：\\(J(\\theta)=\\frac{1}{2}(h_{\\theta}(X)-y)^T(h_{\\theta}(X)-y)\\) 其中， \\[ \\begin{align} X=&amp;\\begin{bmatrix} 1 &amp; x_1^1 &amp; \\cdots &amp; x_n^1 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_1^m &amp; \\cdots &amp; x_n^m \\end{bmatrix}(size:m*(n+1)),\\\\ \\theta=&amp; \\begin{bmatrix}\\theta_0 \\\\ \\theta_1\\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}(size:(n+1)*1),\\\\ y=&amp; \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ \\ y_m \\end{bmatrix}(size:m*1) \\end{align} \\] 要求\\(J(\\theta)\\)的最小值，可以另\\(\\frac{\\partial}{\\partial{\\theta}}{J(\\theta)}=0 \\Rightarrow X^T(X\\theta-y)=0\\)。所以： \\[ \\theta =(X^TX)^{-1}X^Ty \\] 两种方法的差异 梯度下降 | 正规方程 ---- | ---- 需要选择学习率\\(\\alpha\\) | 不需要 需要多次迭代 | 一次运算得出 当特征数量\\(n\\)大时也能较好适用 | 需要计算 \\((X^TX)^{-1}\\)如果特征数量\\(n\\)较大则运算代价大，因为矩阵逆的计算时间复杂度为\\(O(n^3)\\)，通常来说当\\(n\\)小于10000 时还是可以接受的 适用于各种类型的模型 | 只适用于线性模型，不适合逻辑回归模型等其他模型 逻辑回归 逻辑回归一般应用在分类问题中，通过引入逻辑函数(logistic function)\\(g(z)=\\frac{1}{1-e^z}\\)，使得逻辑回归模型的假设函数\\(h_{\\theta}(x)=g(\\theta^Tx)\\)输出在\\([0,1]\\)之间。\\(h(\\theta)\\)的作用是，对于输入标量\\(x\\)，根据给定参数\\(\\theta\\)输出\\(y=1\\)的可能性，即\\(h_{\\theta}(x)=P(y=1|x;\\theta)\\)。 重新定义代价函数: \\(J(\\theta)=\\frac{1}{m}Cost(h_{\\theta}(x),y)\\)，对于线性回归：\\(Cost(h_{\\theta}(x),y)=\\sum_{j=1}^n\\frac{1}{2}(h_{\\theta}(x^i)-y^j)^2\\)；定义逻辑回归中： \\[ Cost(h_{\\theta}(x),y)= \\begin{cases} -log(h_{\\theta}(x)), &amp; \\text{if $y=1$}\\\\ -log(1-h_{\\theta}(x)), &amp; \\text{if $y$=0} \\end{cases} \\] \\(h_{\\theta}(x)\\)与\\(Cost(h_{\\theta}(x),y)\\)之间的关系如图： GV7zsx.jpg 简化为：\\(Cost(h_{\\theta}(x),y)=-[y*log(h_{\\theta}(x))+(1-y)*log(1-h_{\\theta}(x))]\\)，从而： \\[ J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m-[y^i*log(h_{\\theta}(x^i))+(1-y^i)*log(1-h_{\\theta}(x^i))] \\] 与线性回归相同，可以采用梯度下降法来求得最优参数： \\[ \\begin{align} repeat :&amp;\\\\ &amp;\\theta_j := \\theta_j-\\alpha\\frac{\\partial}{\\partial{\\theta_j}}{J(\\theta)},\\ (j=0,1,...,m)\\\\ \\end{align} \\] 其中，\\(\\frac{\\partial}{\\partial{\\theta_j}}{J(\\theta)} = \\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^i)-y^i)x_j^i\\)。 这里，似乎逻辑回归与线性回归算法是相同的，然而实际上由于二者的假设函数不同，所以逻辑回归的梯度下降与线性回归的梯度下降是两种不同的算法。 过拟合问题 解决方案： 丢弃一些不能帮助我们正确预测的特征。手工或者算法选择； 正则化。 保留所有的特征，但是减少参数的大小（magnitude）。 正则化线性回归 正则化线性回归的代价函数为： \\[ J(\\theta) = \\frac{1}{2m}[\\sum_{i=1}^n(h_{\\theta}(x^i)-y^i)^2+\\lambda\\sum_{j=1}^n\\theta_j^2] \\] 由于未对\\(\\theta_0\\)进行正则化，所以梯度下降算法将分两种情形： \\[ \\begin{align} repeat :&amp;\\\\ &amp; \\theta_0 := \\theta_0-\\alpha \\frac{1}{m}\\sum_{i=1}^n(h_{\\theta}(x^i)-y^i)x_0^i \\\\ &amp;\\theta_j := \\theta_j-\\alpha [\\frac{1}{m}\\sum_{i=1}^n(h_{\\theta}(x^i)-y^i)x_j^i+\\frac{\\lambda}{m}\\theta_j],\\ (j=1,...,m)\\\\ \\end{align} \\] 对上面\\(j=1,...,m\\)时的式子整理：\\(\\theta_j=\\theta_j(1-\\alpha\\frac{\\lambda}{m} - \\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^i)-y^i)x^i_j)\\)，可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令\\(\\theta\\)值减少了一个额外的值。 也可以利用正规方程求解： \\[ \\theta =(X^TX+\\lambda \\begin{bmatrix} 0 &amp; &amp; &amp; \\\\ &amp; 1 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; 1 \\end{bmatrix})^{-1}X^Ty, \\ \\text{矩阵大小：(n+1)*(n+1)} \\] 正则化逻辑回归 正则化逻辑回归的代价函数为： \\[ J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m-[y^i*log(h_{\\theta}(x^i))+(1-y^i)*log(1-h_{\\theta}(x^i))]+\\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta^2_j \\] 梯度下降算法： \\[ \\begin{align} repeat :&amp;\\\\ &amp; \\theta_0 := \\theta_0-\\alpha \\frac{1}{m}\\sum_{i=1}^n(h_{\\theta}(x^i)-y^i)x_0^i \\\\ &amp;\\theta_j := \\theta_j-\\alpha [\\frac{1}{m}\\sum_{i=1}^n(h_{\\theta}(x^i)-y^i)x_j^i+\\frac{\\lambda}{m}\\theta_j],\\ (j=1,...,m)\\\\ \\end{align} \\] 处理梯度下降法外，还有共轭梯度（Conjugate Gradient），局部优化法(Broyden fletcher goldfarb shann,BFGS)和有限内存局部优化法(LBFGS)等高级算法来求解最优参数，在matlab或octave中可以直接应用这些方法。 参考： 斯坦福大学2014机器学习教程中文笔记目录","link":"/2019/12/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"title":"使用Hexo建立博客","text":"建站 安装hexo之后，便可以开始建站 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install Private repository The following instruction is adapted from one-command deployment page. 1Install hexo-deployer-git. Add the following configurations to _config.yml, (remove existing lines if any) 12345deploy: type: git repo: https://github.com/&lt;username&gt;/&lt;project&gt; # example, https://github.com/hexojs/hexojs.github.io branch: master 1Run hexo clean &amp;&amp; hexo deploy. Check the webpage at username.github.io. Project page 如果你更希望你的站点部署在 &lt;你的 GitHub 用户名&gt;.github.io 的子目录中，你的 repository 需要直接命名为子目录的名字，这样你的站点可以通过 https://&lt;你的 GitHub 用户名&gt;.github.io/&lt;repository 的名字&gt; 访问。你需要检查你的 Hexo 配置文件，将 url 修改为 https://&lt;你的 GitHub 用户名&gt;.github.io/&lt;repository 的名字&gt;、将 root 的值修改为 /&lt;repository 的名字&gt;/ 配置hexo-deployer-git _config.yml设置 123456789101112131415161718192021222324252627282930313233343536373839# You can use this:deploy: type: git repo: &lt;repository url&gt; branch: [branch] message: [message] name: [git user] email: [git email] extend_dirs: [extend directory] ignore_hidden: false # default is true ignore_pattern: regexp # whatever file that matches the regexp will be ignored when deploying# or this:deploy: type: git message: [message] repo: # both formats are acceptable [repo host name]: &lt;repository url&gt;[,branch] [another repo]: url: &lt;repository url&gt; branch: [branch] token: [$ENV_TOKEN] extend_dirs: - [extend directory] - [another extend directory] ignore_hidden: public: false [extend directory]: true [another extend directory]: false ignore_pattern: [folder]: regexp # or you could specify the ignore_pattern under a certain directory#names of repo do not matter, and can be omitted if there is only one repo in your config:deploy: repo: url: &lt;repository url&gt; token: [$ENV_TOKEN] # ...your other configs","link":"/2019/11/09/%E4%BD%BF%E7%94%A8Hexo%E5%BB%BA%E7%AB%8B%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"技术","slug":"技术","link":"/tags/%E6%8A%80%E6%9C%AF/"}],"categories":[]}